{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-12 18:08:05.421 Python[75648:1287068] apply_selection_policy_once: avoid use of removable GPUs (via org.python.python:GPUSelectionPolicy->avoidRemovable)\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from transformers import LlamaTokenizer, LlamaModel\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "import requests\n",
    "import os.path\n",
    "import hashlib\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "user_path = \".\"\n",
    "data_path = \"/Users/shijia/PycharmProjects/GPTprompt/\"\n",
    "cache_path = \"/Users/shijia/PycharmProjects/GPTprompt/cache/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
    "API_KEY = \"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\"\n",
    "openai.api_key = API_KEY\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6))\n",
    "def get_embedding(text: str, model=\"text-embedding-ada-002\") -> list[float]:\n",
    "    return openai.Embedding.create(input=[text], model=model)[\"data\"][0][\"embedding\"]\n",
    "\n",
    "\n",
    "# embedding = get_embedding(\"Your text goes here\", model=\"text-embedding-ada-002\")\n",
    "# print(embedding, type(embedding), len(embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# verbs = ['dream', 'help', 'kick', 'laugh', 'pray', 'sneeze', 'throw']\n",
    "# dfs_dict = {}\n",
    "\n",
    "# for verb in verbs:\n",
    "#     verb_df = pd.read_csv(data_path + f'{verb}_annotated.csv', na_filter=False)\n",
    "#     shuffled_verb_df = shuffle(verb_df)\n",
    "#     shuffled_verb_df['prediction'] = shuffled_verb_df['prediction'].apply(lambda x: str(x)[0].lower())\n",
    "\n",
    "#     # Deduplicate by 'sentence' value\n",
    "#     shuffled_verb_df['lowercase_sentence'] = shuffled_verb_df['sentence'].str.strip().str.lower()\n",
    "#     deduplicated_verb_df = shuffled_verb_df.drop_duplicates(subset='lowercase_sentence')\n",
    "\n",
    "#     # Remove the 'lowercase_sentence' column before storing in dfs_dict\n",
    "#     deduplicated_verb_df = deduplicated_verb_df.drop('lowercase_sentence', axis=1)\n",
    "\n",
    "#     dfs_dict[verb] = deduplicated_verb_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cache_filename(sentence):\n",
    "    return hashlib.md5(sentence.encode('utf-8')).hexdigest() + '.pkl'\n",
    "\n",
    "def get_embeddings_for_sentences(sentences_list, save_dir_path=None):\n",
    "\n",
    "    existing_sentences_set = set()\n",
    "    if save_dir_path is not None:\n",
    "        os.makedirs(save_dir_path, exist_ok=True)\n",
    "        files = os.listdir(save_dir_path)\n",
    "        existing_sentences_set = set(files)\n",
    "\n",
    "    new_sentences_list = [sentence for sentence in sentences_list if cache_filename(sentence) not in existing_sentences_set]\n",
    "\n",
    "    print(f'Getting embeddings for {len(new_sentences_list)} sentences...')\n",
    "\n",
    "    for sentence in new_sentences_list:\n",
    "        sentence_embedding = np.array(get_embedding(sentence))\n",
    "        file_path = os.path.join(save_dir_path, cache_filename(sentence))\n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump(sentence_embedding, f)\n",
    "\n",
    "\n",
    "\n",
    "def get_embeddings_for_sentence(sentence, embeddings_dir_path):    \n",
    "    filename = cache_filename(sentence) # filename = cache_filename(sentence['sentence'])\n",
    "    file_path = os.path.join(embeddings_dir_path, filename)\n",
    "    with open(file_path, 'rb') as f:\n",
    "        embeddings_of_sentence = pickle.load(f)\n",
    "\n",
    "    return embeddings_of_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adjective</th>\n",
       "      <th>label</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>afraid</td>\n",
       "      <td>causal excess</td>\n",
       "      <td>One man was so afraid that he camped in the mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>afraid</td>\n",
       "      <td>non-causal</td>\n",
       "      <td>He was so afraid that rival loyalist inmates w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>amazed</td>\n",
       "      <td>causal excess</td>\n",
       "      <td>He was so amazed (that) he forgot to thank me .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>amazed</td>\n",
       "      <td>causal</td>\n",
       "      <td>Marie is (so) amazed that he came up with that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>angry</td>\n",
       "      <td>causal excess</td>\n",
       "      <td>Her uncle was so angry that he sent her to liv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>skeptical</td>\n",
       "      <td>causal excess</td>\n",
       "      <td>I was so skeptical that I used frozen blueberr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>startled</td>\n",
       "      <td>causal excess</td>\n",
       "      <td>I was so startled that I began to laugh and al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>startled</td>\n",
       "      <td>causal</td>\n",
       "      <td>I am so startled that no one seemed to have a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>uncertain</td>\n",
       "      <td>causal excess</td>\n",
       "      <td>The character Kate was so uncertain that her b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>uncertain</td>\n",
       "      <td>non-causal</td>\n",
       "      <td>I am so uncertain that there were really enoug...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>330 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     adjective          label  \\\n",
       "0       afraid  causal excess   \n",
       "1       afraid     non-causal   \n",
       "2       amazed  causal excess   \n",
       "3       amazed         causal   \n",
       "4        angry  causal excess   \n",
       "..         ...            ...   \n",
       "325  skeptical  causal excess   \n",
       "326   startled  causal excess   \n",
       "327   startled         causal   \n",
       "328  uncertain  causal excess   \n",
       "329  uncertain     non-causal   \n",
       "\n",
       "                                              sentence  \n",
       "0    One man was so afraid that he camped in the mi...  \n",
       "1    He was so afraid that rival loyalist inmates w...  \n",
       "2      He was so amazed (that) he forgot to thank me .  \n",
       "3    Marie is (so) amazed that he came up with that...  \n",
       "4    Her uncle was so angry that he sent her to liv...  \n",
       "..                                                 ...  \n",
       "325  I was so skeptical that I used frozen blueberr...  \n",
       "326  I was so startled that I began to laugh and al...  \n",
       "327  I am so startled that no one seemed to have a ...  \n",
       "328  The character Kate was so uncertain that her b...  \n",
       "329  I am so uncertain that there were really enoug...  \n",
       "\n",
       "[330 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('combi_3.xlsx')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'causal excess': 0, 'non-causal': 1, 'causal': 2, 'only causal excess': 3}\n"
     ]
    }
   ],
   "source": [
    "def replace_parentheses(text):\n",
    "    # Use regular expressions to delete the \"()\" added manually\n",
    "    text = re.sub(r'\\(so\\)', 'so', text)\n",
    "    text = re.sub(r'\\(that\\)', 'that', text)\n",
    "    return text\n",
    "\n",
    "sentences = [replace_parentheses(s) for s in df[\"sentence\"]]\n",
    "category_column = df['label']\n",
    "unique_categories = category_column.unique()\n",
    "category_to_label = {category: label for label, category in enumerate(unique_categories)}\n",
    "labels = category_column.map(category_to_label).to_numpy()\n",
    "adjectives = df['adjective']\n",
    "\n",
    "print(category_to_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting embeddings for 94 sentences...\n"
     ]
    }
   ],
   "source": [
    "# all_construction_sentences = set([sentence['sentence'] for df in dfs_dict.values() for _, sentence in df.iterrows()])\n",
    "all_sentence_embeddings = get_embeddings_for_sentences(sentences, save_dir_path=cache_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = [get_embeddings_for_sentence(s, cache_path) for s in sentences]\n",
    "# print(len(embeddings[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create an empty dictionary to store the word embeddings\n",
    "# embeddings_by_focus_word = {}\n",
    "\n",
    "# # Iterate over each idiom and its associated constructions and focus words\n",
    "# for focus_word, examples in tqdm(dfs_dict.items(), desc='Construction Embeddings'):\n",
    "#     print(focus_word, len(examples))\n",
    "        \n",
    "#     embeddings_for_this_focus_word = []\n",
    "\n",
    "#     # Iterate over each example sentence\n",
    "#     for i, sentence in tqdm(examples.iterrows(), desc='Examples', leave=False):\n",
    "\n",
    "#         # Get the embeddings for the focus word in this sentence\n",
    "#         try:\n",
    "#             embeddings = get_embeddings_for_sentence(sentence, cache_path)\n",
    "#         except Exception as e:\n",
    "#             print(e)\n",
    "#             raise Exception\n",
    "#             continue\n",
    "\n",
    "        \n",
    "\n",
    "#         # Add the embeddings to the list\n",
    "#         embeddings_for_this_focus_word.append((sentence,embeddings))\n",
    "\n",
    "#     # Add the dictionary of embeddings for this focus word to the dictionary for this construction\n",
    "#     embeddings_by_focus_word[focus_word] = embeddings_for_this_focus_word \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tiktoken\n",
    "\n",
    "def bow_baseline(train_sentences, test_sentences, train_labels, test_labels):\n",
    "    # Tokenize sentences\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "    \n",
    "    # Build bag-of-words representation\n",
    "    train_bows = [Counter(encoding.encode(sentence)) for sentence in train_sentences]\n",
    "    test_bows = [Counter(encoding.encode(sentence)) for sentence in test_sentences]\n",
    "    \n",
    "    # Define a global vocabulary - a set of all unique tokens in the training set\n",
    "    vocabulary = set()\n",
    "    for bow in train_bows:\n",
    "        vocabulary.update(bow.keys())\n",
    "    vocab_list = sorted(list(vocabulary))\n",
    "    vocab_index = {token: i for i, token in enumerate(vocab_list)}\n",
    "    \n",
    "    # Convert Counters to arrays\n",
    "    bow_matrix = lambda bow : np.array([bow.get(token, 0) for token in vocab_list])  # Use this function to convert bow into matrix form\n",
    "    \n",
    "    train_bow_matrix = np.array([bow_matrix(bow) for bow in train_bows])\n",
    "    test_bow_matrix = np.array([bow_matrix(bow) for bow in test_bows])\n",
    "    \n",
    "    # Train logistic regression classifier\n",
    "    clf = LogisticRegression().fit(train_bow_matrix, train_labels)\n",
    "    \n",
    "    # Calculate and return train and test accuracies\n",
    "    train_accuracy = accuracy_score(train_labels, clf.predict(train_bow_matrix))\n",
    "    test_accuracy = accuracy_score(test_labels, clf.predict(test_bow_matrix))\n",
    "    \n",
    "    return train_accuracy, test_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from scipy import stats\n",
    "\n",
    "def get_lr_acc(embeddings, labels, prepositions, sentence_texts):\n",
    "\n",
    "    accuracies = []\n",
    "    weights = []\n",
    "\n",
    "    test_accuracies = []\n",
    "    bow_baseline_test_accuracies = []\n",
    "\n",
    "    for preposition in set(prepositions):\n",
    "        # Create a boolean mask for selecting sentences with this preposition\n",
    "        mask = [preposition == p for p in prepositions]\n",
    "\n",
    "        # Split the data into a training set and a test set\n",
    "        X_train = [e for i, e in enumerate(embeddings) if not mask[i]]\n",
    "        y_train = [l for i, l in enumerate(labels) if not mask[i]]\n",
    "        sentence_texts_train = [t for i, t in enumerate(sentence_texts) if not mask[i]]\n",
    "       \n",
    "        X_test = [e for i, e in enumerate(embeddings) if mask[i]]\n",
    "        y_test = [l for i, l in enumerate(labels) if mask[i]]\n",
    "        sentence_texts_test = [t for i, t in enumerate(sentence_texts) if mask[i]]\n",
    "\n",
    "        df = pd.read_excel('only causal excess.xlsx')\n",
    "        sentence_texts_test_o = df[\"sentence\"]\n",
    "        X_test_o = [get_embeddings_for_sentence(s, cache_path) for s in sentences]\n",
    "        y_test_o = np.array([0]*len(X_test_o))\n",
    "        \n",
    "\n",
    "        clf = LogisticRegression(solver='liblinear')\n",
    "\n",
    "        if len(set(y_train)) > 1:\n",
    "\n",
    "            # Balance the dataset\n",
    "            resampler = RandomUnderSampler(sampling_strategy='majority')\n",
    "            X_train_resampled, y_train_resampled = resampler.fit_resample(np.array(X_train), np.array(y_train))\n",
    "\n",
    "            # Train the model on the resampled data\n",
    "            clf.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "            # Compute the training accuracy\n",
    "            train_predictions = clf.predict(X_train_resampled)\n",
    "            train_accuracy = accuracy_score(y_train_resampled, train_predictions)\n",
    "\n",
    "            # Compute the test accuracy\n",
    "            test_predictions = clf.predict(X_test)\n",
    "            test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "            \n",
    "            # Compute the test accuracy of only causal excess\n",
    "            test_predictions_o = clf.predict(X_test_o)\n",
    "            test_accuracy_o = accuracy_score(y_test_o, test_predictions_o)\n",
    "\n",
    "            # Compute the majority class accuracy on the resampled data\n",
    "            majority_class = Counter(y_train_resampled).most_common(1)[0][0]\n",
    "            majority_class_predictions = [majority_class] * len(y_test)\n",
    "            majority_class_accuracy = accuracy_score(y_test, majority_class_predictions)\n",
    "\n",
    "            # Use resampled data in bow_baseline\n",
    "            sentence_texts_train_resampled = [t for i, t in enumerate(sentence_texts_train) if i in resampler.sample_indices_]\n",
    "            bow_baseline_train_acc, bow_baseline_test_acc = bow_baseline(sentence_texts_train_resampled, sentence_texts_test, y_train_resampled, y_test)\n",
    "\n",
    "\n",
    "        accuracies.append((train_accuracy, test_accuracy, majority_class_accuracy, bow_baseline_train_acc, bow_baseline_test_acc, test_accuracy_o))\n",
    "        weights.append(sum(mask))\n",
    "\n",
    "        test_accuracies.append(test_accuracy)\n",
    "        bow_baseline_test_accuracies.append(bow_baseline_test_acc)\n",
    "\n",
    "    accuracies = np.array(accuracies)\n",
    "    weights = np.array(weights)\n",
    "    mean_accuracies = np.average(accuracies, axis=0, weights=weights)\n",
    "    std_accuracies = np.sqrt(np.average((accuracies - mean_accuracies)**2, axis=0, weights=weights))\n",
    "\n",
    "    t_stat, p_value = stats.ttest_rel(test_accuracies, bow_baseline_test_accuracies)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    result = {\n",
    "        'mean_train_accuracy': mean_accuracies[0],\n",
    "        'mean_test_accuracy': mean_accuracies[1],\n",
    "        'mean_test_accuracy_only_causal_excess': mean_accuracies[5],\n",
    "        'mean_majority_class_accuracy': mean_accuracies[2],\n",
    "        'mean_bow_baseline_train_accuracy': mean_accuracies[3],\n",
    "        'mean_bow_baseline_test_accuracy': mean_accuracies[4],\n",
    "        'std_train_accuracy': std_accuracies[0],\n",
    "        'std_test_accuracy': std_accuracies[1],\n",
    "        'std_majority_class_accuracy': std_accuracies[2],\n",
    "        'std_bow_baseline_train_accuracy': std_accuracies[3],\n",
    "        'std_bow_baseline_test_accuracy': std_accuracies[4]\n",
    "    }\n",
    "\n",
    "    result['t_stat'] = t_stat\n",
    "    result['p_value'] = p_value\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74\n",
      "58\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "embed0 = [embed for label, embed in zip(labels, embeddings) if label == 0]\n",
    "embed1 = [embed for label, embed in zip(labels, embeddings) if label == 1]\n",
    "embed2 = [embed for label, embed in zip(labels, embeddings) if label == 2]\n",
    "adj0 = [adj for label, adj in zip(labels, adjectives) if label == 0]\n",
    "adj1 = [adj for label, adj in zip(labels, adjectives) if label == 1]\n",
    "adj2 = [adj for label, adj in zip(labels, adjectives) if label == 2]\n",
    "sent0 = [sent for label, sent in zip(labels, sentences) if label == 0]\n",
    "sent1 = [sent for label, sent in zip(labels, sentences) if label == 1]\n",
    "sent2 = [sent for label, sent in zip(labels, sentences) if label == 2]\n",
    "label0 = np.array([0]*len(embed0))\n",
    "label1 = np.array([1]*len(embed1))\n",
    "label2 = np.array([2]*len(embed2))\n",
    "label01 = np.array([0]*len(embed0+embed1))\n",
    "label12 = np.array([1]*len(embed1+embed2))\n",
    "label02 = np.array([0]*len(embed0+embed2))\n",
    "\n",
    "print(len(label0))\n",
    "print(len(label1))\n",
    "print(len(label2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "causal excess vs. causal:\n",
      "mean_train_accuracy: 0.902917560358576\n",
      "mean_test_accuracy: 0.75\n",
      "mean_test_accuracy_only_causal_excess: 0.5753992628992629\n",
      "mean_majority_class_accuracy: 0.5606060606060606\n",
      "mean_bow_baseline_train_accuracy: 1.0\n",
      "mean_bow_baseline_test_accuracy: 0.7196969696969697\n",
      "std_train_accuracy: 0.014425488254879\n",
      "std_test_accuracy: 0.27866021451135636\n",
      "std_majority_class_accuracy: 0.16318681233740917\n",
      "std_bow_baseline_train_accuracy: 0.0\n",
      "std_bow_baseline_test_accuracy: 0.33290261428363366\n",
      "t_stat: 0.6597342077675226\n",
      "p_value: 0.5115005198305822\n"
     ]
    }
   ],
   "source": [
    "result_excess_causal = get_lr_acc(embed0+embed1, np.concatenate((label0,label1)), adj0+adj1, sent0+sent1)\n",
    "print(\"causal excess vs. causal:\")\n",
    "for key, value in result_excess_causal.items():\n",
    "    print(f'{key}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "causal excess vs. non-causal:\n",
      "mean_train_accuracy: 0.9936111111111111\n",
      "mean_test_accuracy: 0.7444444444444445\n",
      "mean_test_accuracy_only_causal_excess: 0.7098348348348348\n",
      "mean_majority_class_accuracy: 0.8222222222222222\n",
      "mean_bow_baseline_train_accuracy: 1.0\n",
      "mean_bow_baseline_test_accuracy: 0.6555555555555556\n",
      "std_train_accuracy: 0.012785323858738958\n",
      "std_test_accuracy: 0.3744955454745047\n",
      "std_majority_class_accuracy: 0.23934065809486693\n",
      "std_bow_baseline_train_accuracy: 0.0\n",
      "std_bow_baseline_test_accuracy: 0.4126098806139364\n",
      "t_stat: 2.320744998320741\n",
      "p_value: 0.023097205853622415\n"
     ]
    }
   ],
   "source": [
    "result_excess_noncausal = get_lr_acc(embed0+embed2, np.concatenate((label0,label2)), adj0+adj2, sent0+sent2)\n",
    "print(\"causal excess vs. non-causal:\")\n",
    "for key, value in result_excess_noncausal.items():\n",
    "    print(f'{key}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "causal vs. non-causal:\n",
      "mean_train_accuracy: 0.985838963963964\n",
      "mean_test_accuracy: 0.7837837837837838\n",
      "mean_test_accuracy_only_causal_excess: 0.0\n",
      "mean_majority_class_accuracy: 0.7837837837837838\n",
      "mean_bow_baseline_train_accuracy: 1.0\n",
      "mean_bow_baseline_test_accuracy: 0.6216216216216216\n",
      "std_train_accuracy: 0.01893979184925208\n",
      "std_test_accuracy: 0.4116634111277789\n",
      "std_majority_class_accuracy: 0.4116634111277789\n",
      "std_bow_baseline_train_accuracy: 0.0\n",
      "std_bow_baseline_test_accuracy: 0.48498266067368523\n",
      "t_stat: 2.805042508243662\n",
      "p_value: 0.006443621274807869\n"
     ]
    }
   ],
   "source": [
    "result_causal_noncausal = get_lr_acc(embed1+embed2, np.concatenate((label1,label2)), adj1+adj2, sent1+sent2)\n",
    "print(\"causal vs. non-causal:\")\n",
    "for key, value in result_causal_noncausal.items():\n",
    "    print(f'{key}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "causal excess vs. others:\n",
      "mean_train_accuracy: 0.9056830803406157\n",
      "mean_test_accuracy: 0.7567567567567568\n",
      "mean_test_accuracy_only_causal_excess: 0.49552593133674194\n",
      "mean_majority_class_accuracy: 0.5\n",
      "mean_bow_baseline_train_accuracy: 1.0\n",
      "mean_bow_baseline_test_accuracy: 0.7905405405405406\n",
      "std_train_accuracy: 0.011892572516039273\n",
      "std_test_accuracy: 0.24990867579603962\n",
      "std_majority_class_accuracy: 0.0\n",
      "std_bow_baseline_train_accuracy: 0.0\n",
      "std_bow_baseline_test_accuracy: 0.2964514995740581\n",
      "t_stat: -0.7986794413608186\n",
      "p_value: 0.4270671475329877\n"
     ]
    }
   ],
   "source": [
    "result_excess_others = get_lr_acc(embed0+embed1+embed2, np.concatenate((label0,label12)), adj0+adj1+adj2, sent0+sent1+sent2)\n",
    "print(\"causal excess vs. others:\")\n",
    "for key, value in result_excess_others.items():\n",
    "    print(f'{key}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "causal vs. others:\n",
      "mean_train_accuracy: 0.8725556318569024\n",
      "mean_test_accuracy: 0.7162162162162162\n",
      "mean_test_accuracy_only_causal_excess: 0.5537801314828342\n",
      "mean_majority_class_accuracy: 0.6081081081081081\n",
      "mean_bow_baseline_train_accuracy: 1.0\n",
      "mean_bow_baseline_test_accuracy: 0.34459459459459457\n",
      "std_train_accuracy: 0.019576272900694823\n",
      "std_test_accuracy: 0.2736277936664404\n",
      "std_majority_class_accuracy: 0.20583170556388944\n",
      "std_bow_baseline_train_accuracy: 0.0\n",
      "std_bow_baseline_test_accuracy: 0.3067448503463829\n",
      "t_stat: 7.566574284914295\n",
      "p_value: 9.156120685478066e-11\n"
     ]
    }
   ],
   "source": [
    "result_causal_others = get_lr_acc(embed1+embed0+embed2, np.concatenate((label1,label02)), adj1+adj0+adj2, sent1+sent0+sent2)\n",
    "print(\"causal vs. others:\")\n",
    "for key, value in result_causal_others.items():\n",
    "    print(f'{key}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non-causal vs. others:\n",
      "mean_train_accuracy: 0.9841216216216216\n",
      "mean_test_accuracy: 0.777027027027027\n",
      "mean_test_accuracy_only_causal_excess: 0.6924762600438276\n",
      "mean_majority_class_accuracy: 0.8918918918918919\n",
      "mean_bow_baseline_train_accuracy: 1.0\n",
      "mean_bow_baseline_test_accuracy: 0.35135135135135137\n",
      "std_train_accuracy: 0.02221938453292518\n",
      "std_test_accuracy: 0.30911700631615846\n",
      "std_majority_class_accuracy: 0.20583170556388944\n",
      "std_bow_baseline_train_accuracy: 0.0\n",
      "std_bow_baseline_test_accuracy: 0.34611482364501905\n",
      "t_stat: 7.023797450930971\n",
      "p_value: 9.411253469804595e-10\n"
     ]
    }
   ],
   "source": [
    "result_noncausal_others = get_lr_acc(embed2+embed0+embed1, np.concatenate((label2,label01)), adj2+adj0+adj1, sent2+sent0+sent1)\n",
    "print(\"non-causal vs. others:\")\n",
    "for key, value in result_noncausal_others.items():\n",
    "    print(f'{key}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import defaultdict\n",
    "import matplotlib\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "from openTSNE import TSNE\n",
    "from sklearn.metrics import silhouette_score\n",
    "import umap\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.subplots as sp\n",
    "\n",
    "\n",
    "def get_colors(n: int):\n",
    "    # Define a list of 22 distinguishable colors (excluding white)\n",
    "    all_colors = [\n",
    "        'black', 'gray', 'deeppink', 'darkgreen',\n",
    "        'red', 'salmon', 'fuchsia', 'orange',\n",
    "        'gold', 'yellow', 'olive', 'forestgreen',\n",
    "        'dodgerblue', 'saddlebrown', 'cyan', 'royalblue',\n",
    "        'blue', 'indigo', 'violet', 'pink',\n",
    "        'maroon', 'teal', 'limegreen'\n",
    "    ]\n",
    "\n",
    "    if n > len(all_colors):\n",
    "        print(n)\n",
    "        raise ValueError(f\"Cannot select more than {len(all_colors)} distinguishable colors.\")\n",
    "\n",
    "    # Return the first 'n' colors from the list\n",
    "    return all_colors[:n]\n",
    "\n",
    "\n",
    "def get_new_color(sense):\n",
    "    if sense == 't':\n",
    "        color = 'forestgreen'\n",
    "    elif sense == 'f':\n",
    "        color = 'orangered'\n",
    "    return color\n",
    "\n",
    "def scatter_layer(projection_method, example_sentences, focus_word, color_prepositions):\n",
    "    results = {}\n",
    "    displayed_prepositions = set()\n",
    "    class_labels = ['t', 'f']\n",
    "\n",
    "    def get_label(example_sentence):\n",
    "        label = example_sentence['annotation'] if example_sentence['annotation'] in class_labels else example_sentence['prediction']\n",
    "        return label\n",
    "    \n",
    "    input_data = np.array([e[1] for e in example_sentences])\n",
    "\n",
    "    if projection_method == 'TSNE':\n",
    "        tsne = TSNE(n_components=2, perplexity=5)\n",
    "        embeddings_reduced = tsne.fit(input_data)\n",
    "    elif projection_method == 'UMAP':\n",
    "        umap_model = umap.UMAP(n_components=2, n_neighbors=200, min_dist=1)\n",
    "        embeddings_reduced = umap_model.fit_transform(input_data)\n",
    "    else:\n",
    "        raise Exception(f'Unknown projection method: {projection_method}')\n",
    "\n",
    "\n",
    "    labels = [get_label(e[0]) for e in example_sentences] # Assuming 'e[2]' is the 'sense'\n",
    "    prepositions = [e[0]['preposition'] for e in example_sentences]\n",
    "    texts = [e[0]['sentence'] for e in example_sentences]\n",
    "\n",
    "    results = get_lr_acc(input_data, labels, prepositions, texts)\n",
    "\n",
    "    # Define the width for left-aligned text.\n",
    "    width = 40\n",
    "\n",
    "    # Filtering results to exclude keys containing 'train'\n",
    "    mean_results_text = '<br>'.join([f'{k.replace(\"mean_\", \"\")}: {v:.2f}'.ljust(width)\n",
    "                                    for k, v in results.items()\n",
    "                                    if k.startswith(\"mean\") and \"train\" not in k])\n",
    "\n",
    "    # Add t-statistic and p-value at the end\n",
    "    stats_results_text = '<br>'.join([f'{k}: {v:.2f}'.ljust(width)\n",
    "                                    for k, v in results.items()\n",
    "                                    if k in [\"t_stat\", \"p_value\"]])\n",
    "\n",
    "    results_text = mean_results_text + \"<br>\" + stats_results_text\n",
    "\n",
    "\n",
    "    fig = sp.make_subplots(rows=1, cols=1)\n",
    "\n",
    "    padding = 5\n",
    "    x_min, x_max = np.min(embeddings_reduced[:, 0]), np.max(embeddings_reduced[:, 0])\n",
    "    y_min, y_max = np.min(embeddings_reduced[:, 1]), np.max(embeddings_reduced[:, 1])\n",
    "    x_range = [x_min - padding, x_max + padding]\n",
    "    y_range = [y_min - padding, y_max + padding]\n",
    "\n",
    "    # Update the x and y axes ranges with the determined range\n",
    "\n",
    "    fig.update_xaxes(range=x_range, showticklabels=False)\n",
    "    fig.update_yaxes(range=y_range, showticklabels=False)\n",
    "    \n",
    "\n",
    "    # Initialize a dictionary to store the data for each sense\n",
    "    plot_data = defaultdict(lambda: defaultdict(lambda: {\"x\": [], \"y\": [], \"text\": []}))\n",
    "    for i, (reduced_embedding, label, preposition, text) in enumerate(zip(embeddings_reduced, labels, prepositions, texts)):\n",
    "        # Collect the data for each sense\n",
    "        plot_data[label][preposition][\"x\"].append(reduced_embedding[0])\n",
    "        plot_data[label][preposition][\"y\"].append(reduced_embedding[1])\n",
    "        plot_data[label][preposition][\"text\"].append(text)\n",
    "\n",
    "\n",
    "    prepositions_set = set(prepositions)\n",
    "    preposition_colors = {p: c for p, c in zip(prepositions_set, get_colors(len(prepositions_set)))}\n",
    "\n",
    "    # Create a Scatter plot for each sense using the collected data\n",
    "    for i, (sense, preposition_data) in enumerate(plot_data.items()):\n",
    "        color = get_new_color(sense)\n",
    "\n",
    "        symbol = 'circle' if sense == 't' else 'x'\n",
    "        name = sense\n",
    "\n",
    "\n",
    "        #sort preposition data by preposition\n",
    "        preposition_data = dict(sorted(preposition_data.items(), key=lambda item: item[0]))\n",
    "\n",
    "        for preposition, data in preposition_data.items():\n",
    "            if color_prepositions:\n",
    "                color = preposition_colors[preposition]\n",
    "                name = preposition\n",
    "\n",
    "            show_legend_preposition = name not in displayed_prepositions\n",
    "            displayed_prepositions.add(name)\n",
    "            \n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=data[\"x\"],\n",
    "                    y=data[\"y\"],\n",
    "                    mode='markers',\n",
    "                    marker=dict(color=color, symbol=symbol),\n",
    "                    text=data[\"text\"],\n",
    "                    hovertemplate='%{text}<extra></extra>',\n",
    "                    showlegend=show_legend_preposition,\n",
    "                    name=name\n",
    "                )\n",
    "            )     \n",
    "    \n",
    "    # Filtering results to exclude keys containing 'train'\n",
    "    mean_results_text = ', '.join([f'{k.replace(\"mean_\", \"\")}: {v:.2f}' \n",
    "                                for k, v in results.items() \n",
    "                                if k.startswith(\"mean\") and \"train\" not in k])\n",
    "\n",
    "    # Add t-statistic and p-value at the end\n",
    "    stats_results_text = ', '.join([f'{k}: {v:.5f}' \n",
    "                                    for k, v in results.items() \n",
    "                                    if k in [\"t_stat\", \"p_value\"]])\n",
    "\n",
    "    # Combine the results text into one string\n",
    "    results_text = f'{mean_results_text}<br>{stats_results_text}'\n",
    "\n",
    "    fig.update_layout(\n",
    "        legend=dict(\n",
    "            orientation='h',  # horizontal layout of items\n",
    "            yanchor='top',\n",
    "            y=0.95,  # Adjust this as per your requirement\n",
    "            xanchor='left',\n",
    "            x=0.05,  # Adjust this as per your requirement\n",
    "            font=dict(\n",
    "                size=12,  # Increase font size for legend\n",
    "            ),\n",
    "        ),\n",
    "        autosize=False,\n",
    "        margin=dict(\n",
    "            l=0,  # left margin\n",
    "            r=0,  # right margin\n",
    "            b=0,  # bottom margin\n",
    "            t=0,  # top margin\n",
    "            pad=0  # padding\n",
    "        ),\n",
    "        width=1150, \n",
    "        height=750\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "    #save plot as pdf\n",
    "    fig.write_image(f\"plots/openai_{focus_word}_{color_prepositions}.pdf\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "# Define the colors for the sentences and senses\n",
    "# sense_colors = ['red', 'green', 'blue', 'orange', 'purple', 'pink', 'brown', 'gray', 'olive', 'teal']\n",
    "projection_method = 'TSNE' # 'TSNE' or 'UMAP'\n",
    "focus_words_per_construction = 5\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Iterate over each construction and its associated embeddings\n",
    "all_results = {}\n",
    "for focus_word, sentences_for_focus_word in embeddings_by_focus_word.items():\n",
    "    if focus_word == \"throw\" or focus_word == \"kick\": continue\n",
    "    all_results[focus_word] = scatter_layer(projection_method, sentences_for_focus_word, focus_word, False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for verb, verb_results in all_results.items():\n",
    "    print(verb)\n",
    "    print(f\"Train Acc: {verb_results['mean_train_accuracy']*100:.2f} +- {verb_results['std_train_accuracy']*100:.2f}\")\n",
    "    print(f\"Test Acc: {verb_results['mean_test_accuracy']*100:.2f} +- {verb_results['std_test_accuracy']*100:.2f}\")\n",
    "    print(f\"BOW Train Acc: {verb_results['mean_bow_baseline_train_accuracy']*100:.2f} +- {verb_results['std_bow_baseline_train_accuracy']*100:.2f}\")\n",
    "    print(f\"BOW Test Acc: {verb_results['mean_bow_baseline_test_accuracy']*100:.2f} +- {verb_results['std_bow_baseline_test_accuracy']*100:.2f}\")\n",
    "    print(f\"t-statistic: {verb_results['t_stat']:.2f}\")\n",
    "    print(f\"p-value: {verb_results['p_value']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# Set a seaborn theme for nicer plots\n",
    "sns.set_theme()\n",
    "\n",
    "all_verbs = all_lr_results[0].keys()\n",
    "\n",
    "class_labels = ['t', 'f']\n",
    "\n",
    "averaged_scores = {letter: [] for letter in class_labels}  \n",
    "averaged_baselines = {letter: [] for letter in class_labels}\n",
    "\n",
    "averaged_scores_std = {letter: [] for letter in class_labels}  \n",
    "averaged_baselines_std = {letter: [] for letter in class_labels}\n",
    "\n",
    "for verb in all_verbs:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.title(f'Graph for {verb}', fontsize=20)\n",
    "    verb_freq = len(verb_data[verb])\n",
    "    total_scores = {letter: [0] * len(all_lr_results) for letter in class_labels}\n",
    "    total_baselines = {letter: [0] * len(all_lr_results) for letter in class_labels}\n",
    "\n",
    "    total_scores_std = {letter: [0] * len(all_lr_results) for letter in class_labels}\n",
    "    total_baselines_std = {letter: [0] * len(all_lr_results) for letter in class_labels}\n",
    "\n",
    "    for letter in class_labels:\n",
    "        relevant_numbers = [layer[verb][letter] for layer in all_lr_results]\n",
    "\n",
    "        test_scores = [number[0][1] for number in relevant_numbers]\n",
    "        baselines = [number[0][2] for number in relevant_numbers]\n",
    "        test_scores_std = [number[1][1] for number in relevant_numbers]\n",
    "        baselines_std = [number[1][2] for number in relevant_numbers]\n",
    "\n",
    "        total_scores[letter] = [total_scores[letter][i] + test_scores[i] * verb_freq for i in range(len(all_lr_results))]\n",
    "        total_baselines[letter] = [total_baselines[letter][i] + baselines[i] * verb_freq for i in range(len(all_lr_results))]\n",
    "\n",
    "        total_scores_std[letter] = [total_scores_std[letter][i] + test_scores_std[i] * verb_freq for i in range(len(all_lr_results))]\n",
    "        total_baselines_std[letter] = [total_baselines_std[letter][i] + baselines_std[i] * verb_freq for i in range(len(all_lr_results))]\n",
    "\n",
    "        plt.errorbar(range(len(all_lr_results)), test_scores, yerr=test_scores_std, fmt='-o', label=letter, color=get_new_color(letter))\n",
    "        plt.hlines(baselines[0], 0, len(all_lr_results)-1, colors=get_new_color(letter), linestyles='dashed', label=f'baseline_{letter}')\n",
    "\n",
    "    for letter in class_labels:\n",
    "        averaged_scores[letter].append([total_scores[letter][i] / verb_freq for i in range(len(all_lr_results))])\n",
    "        averaged_baselines[letter].append(total_baselines[letter][0] / verb_freq)\n",
    "        averaged_scores_std[letter].append([total_scores_std[letter][i] / verb_freq for i in range(len(all_lr_results))])\n",
    "        averaged_baselines_std[letter].append(total_baselines_std[letter][0] / verb_freq)\n",
    "\n",
    "    plt.ylim(0, 1)\n",
    "    plt.xlabel('Layers', fontsize=16)\n",
    "    plt.ylabel('Value', fontsize=16)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Plot the averaged scores and baselines for each letter\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.title('Averaged Scores', fontsize=20)\n",
    "for letter in class_labels:\n",
    "    averaged_scores_letter = averaged_scores[letter]\n",
    "    averaged_baselines_letter = averaged_baselines[letter]\n",
    "    averaged_scores_std_letter = averaged_scores_std[letter]\n",
    "    averaged_baselines_std_letter = averaged_baselines_std[letter]\n",
    "    \n",
    "    averaged_scores_mean = [np.mean([score[i] for score in averaged_scores_letter]) for i in range(len(all_lr_results))]\n",
    "    averaged_baselines_mean = np.mean(averaged_baselines_letter)\n",
    "    \n",
    "    averaged_scores_std_mean = [np.mean([score_std[i] for score_std in averaged_scores_std_letter]) for i in range(len(all_lr_results))]\n",
    "    averaged_baselines_std_mean = np.mean(averaged_baselines_std_letter)\n",
    "\n",
    "    plt.errorbar(range(len(all_lr_results)), averaged_scores_mean, yerr=averaged_scores_std_mean, fmt='-o', label=letter, color=get_new_color(letter))\n",
    "    plt.hlines(averaged_baselines_mean, 0, len(all_lr_results)-1, colors=get_new_color(letter), linestyles='dashed', label=f'Baseline {letter}')\n",
    "\n",
    "plt.ylim(0, 1)\n",
    "plt.xlabel('Layers', fontsize=16)\n",
    "plt.ylabel('Averaged Value', fontsize=16)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.savefig(f'plots/{verb}_averaged.pdf')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "b26bbf8db02da89a16f0bf6b34287d6bafac7092a1e865ceadcdcc00f0f331ad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
