{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-19 23:49:29.576 Python[46716:4722702] apply_selection_policy_once: avoid use of removable GPUs (via org.python.python:GPUSelectionPolicy->avoidRemovable)\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from transformers import LlamaTokenizer, LlamaModel\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "import requests\n",
    "import os.path\n",
    "import hashlib\n",
    "import spacy\n",
    "import re\n",
    "import random\n",
    "\n",
    "user_path = \".\"\n",
    "data_path = \"/Users/shijia/PycharmProjects/GPTprompt/\"\n",
    "cache_path = \"/Users/shijia/PycharmProjects/GPTprompt/gpt_cache/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
    "API_KEY = \"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\"\n",
    "openai.api_key = API_KEY\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6))\n",
    "def get_embedding(text: str, model=\"text-embedding-ada-002\") -> list[float]:\n",
    "    return openai.Embedding.create(input=[text], model=model)[\"data\"][0][\"embedding\"]\n",
    "\n",
    "# embedding = get_embedding(\"Your text goes here\", model=\"text-embedding-ada-002\")\n",
    "# print(embedding, type(embedding), len(embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cache_filename(sentence):\n",
    "    return hashlib.md5(sentence.encode('utf-8')).hexdigest() + '.pkl'\n",
    "\n",
    "def get_embeddings_for_sentences(sentences_list, save_dir_path=None):\n",
    "\n",
    "    existing_sentences_set = set()\n",
    "    if save_dir_path is not None:\n",
    "        os.makedirs(save_dir_path, exist_ok=True)\n",
    "        files = os.listdir(save_dir_path)\n",
    "        existing_sentences_set = set(files)\n",
    "\n",
    "    new_sentences_list = [sentence for sentence in sentences_list if cache_filename(sentence) not in existing_sentences_set]\n",
    "\n",
    "    print(f'Getting embeddings for {len(new_sentences_list)} sentences...')\n",
    "\n",
    "    for sentence in new_sentences_list:\n",
    "        sentence_embedding = np.array(get_embedding(sentence))\n",
    "        file_path = os.path.join(save_dir_path, cache_filename(sentence))\n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump(sentence_embedding, f)\n",
    "\n",
    "\n",
    "def get_embeddings_for_sentence(sentence, embeddings_dir_path):    \n",
    "    filename = cache_filename(sentence) # filename = cache_filename(sentence['sentence'])\n",
    "    file_path = os.path.join(embeddings_dir_path, filename)\n",
    "    with open(file_path, 'rb') as f:\n",
    "        embeddings_of_sentence = pickle.load(f)\n",
    "\n",
    "    return embeddings_of_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adjective</th>\n",
       "      <th>label</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>afraid</td>\n",
       "      <td>causal excess</td>\n",
       "      <td>One man was so afraid that he camped in the mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>afraid</td>\n",
       "      <td>non-causal</td>\n",
       "      <td>He was so afraid that rival loyalist inmates w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>amazed</td>\n",
       "      <td>causal excess</td>\n",
       "      <td>He was so amazed (that) he forgot to thank me .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>amazed</td>\n",
       "      <td>causal</td>\n",
       "      <td>Marie is (so) amazed that he came up with that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>angry</td>\n",
       "      <td>causal excess</td>\n",
       "      <td>Her uncle was so angry that he sent her to liv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>skeptical</td>\n",
       "      <td>causal excess</td>\n",
       "      <td>I was so skeptical that I used frozen blueberr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>startled</td>\n",
       "      <td>causal excess</td>\n",
       "      <td>I was so startled that I began to laugh and al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>startled</td>\n",
       "      <td>causal</td>\n",
       "      <td>I am so startled that no one seemed to have a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>uncertain</td>\n",
       "      <td>causal excess</td>\n",
       "      <td>The character Kate was so uncertain that her b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>uncertain</td>\n",
       "      <td>non-causal</td>\n",
       "      <td>I am so uncertain that there were really enoug...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>323 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     adjective          label  \\\n",
       "0       afraid  causal excess   \n",
       "1       afraid     non-causal   \n",
       "2       amazed  causal excess   \n",
       "3       amazed         causal   \n",
       "4        angry  causal excess   \n",
       "..         ...            ...   \n",
       "318  skeptical  causal excess   \n",
       "319   startled  causal excess   \n",
       "320   startled         causal   \n",
       "321  uncertain  causal excess   \n",
       "322  uncertain     non-causal   \n",
       "\n",
       "                                              sentence  \n",
       "0    One man was so afraid that he camped in the mi...  \n",
       "1    He was so afraid that rival loyalist inmates w...  \n",
       "2      He was so amazed (that) he forgot to thank me .  \n",
       "3    Marie is (so) amazed that he came up with that...  \n",
       "4    Her uncle was so angry that he sent her to liv...  \n",
       "..                                                 ...  \n",
       "318  I was so skeptical that I used frozen blueberr...  \n",
       "319  I was so startled that I began to laugh and al...  \n",
       "320  I am so startled that no one seemed to have a ...  \n",
       "321  The character Kate was so uncertain that her b...  \n",
       "322  I am so uncertain that there were really enoug...  \n",
       "\n",
       "[323 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('combi_3.xlsx')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'causal excess': 0, 'non-causal': 1, 'causal': 2, 'only causal excess': 3}\n",
      "323\n"
     ]
    }
   ],
   "source": [
    "def replace_parentheses(text):\n",
    "    # Use regular expressions to delete the \"()\" added manually\n",
    "    text = re.sub(r'\\(so\\)', 'so', text)\n",
    "    text = re.sub(r'\\(that\\)', 'that', text)\n",
    "    return text\n",
    "\n",
    "df[\"sentence\"] = [replace_parentheses(s) for s in df[\"sentence\"]]\n",
    "category_column = df['label']\n",
    "unique_categories = category_column.unique()\n",
    "category_to_label = {category: label for label, category in enumerate(unique_categories)}\n",
    "df['gold_label'] = category_column.map(category_to_label).to_numpy()\n",
    "adjectives = df['adjective']\n",
    "\n",
    "print(category_to_label)\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting embeddings for 0 sentences...\n"
     ]
    }
   ],
   "source": [
    "# all_construction_sentences = set([sentence['sentence'] for df in dfs_dict.values() for _, sentence in df.iterrows()])\n",
    "all_sentence_embeddings = get_embeddings_for_sentences(df[\"sentence\"], save_dir_path=cache_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "323\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adjective</th>\n",
       "      <th>label</th>\n",
       "      <th>sentence</th>\n",
       "      <th>gold_label</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>afraid</td>\n",
       "      <td>causal excess</td>\n",
       "      <td>One man was so afraid that he camped in the mi...</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.02166154235601425, -0.006525898817926645, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>afraid</td>\n",
       "      <td>non-causal</td>\n",
       "      <td>He was so afraid that rival loyalist inmates w...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.00969354435801506, -0.0024283453822135925,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>amazed</td>\n",
       "      <td>causal excess</td>\n",
       "      <td>He was so amazed that he forgot to thank me .</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.015139495022594929, -0.0003121292102150619...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>amazed</td>\n",
       "      <td>causal</td>\n",
       "      <td>Marie is so amazed that he came up with that s...</td>\n",
       "      <td>2</td>\n",
       "      <td>[-0.01651465706527233, 0.002538929460570216, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>angry</td>\n",
       "      <td>causal excess</td>\n",
       "      <td>Her uncle was so angry that he sent her to liv...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.003403774229809642, -0.01876366324722767, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>skeptical</td>\n",
       "      <td>causal excess</td>\n",
       "      <td>I was so skeptical that I used frozen blueberr...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0014925083378329873, -0.03001386672258377, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>startled</td>\n",
       "      <td>causal excess</td>\n",
       "      <td>I was so startled that I began to laugh and al...</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.03688151389360428, -0.010876656509935856, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>startled</td>\n",
       "      <td>causal</td>\n",
       "      <td>I am so startled that no one seemed to have a ...</td>\n",
       "      <td>2</td>\n",
       "      <td>[-0.015010016970336437, -0.031310923397541046,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>uncertain</td>\n",
       "      <td>causal excess</td>\n",
       "      <td>The character Kate was so uncertain that her b...</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.013836754485964775, 0.004289925564080477, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>uncertain</td>\n",
       "      <td>non-causal</td>\n",
       "      <td>I am so uncertain that there were really enoug...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.010967683047056198, -0.0024471436627209187...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>323 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     adjective          label  \\\n",
       "0       afraid  causal excess   \n",
       "1       afraid     non-causal   \n",
       "2       amazed  causal excess   \n",
       "3       amazed         causal   \n",
       "4        angry  causal excess   \n",
       "..         ...            ...   \n",
       "318  skeptical  causal excess   \n",
       "319   startled  causal excess   \n",
       "320   startled         causal   \n",
       "321  uncertain  causal excess   \n",
       "322  uncertain     non-causal   \n",
       "\n",
       "                                              sentence  gold_label  \\\n",
       "0    One man was so afraid that he camped in the mi...           0   \n",
       "1    He was so afraid that rival loyalist inmates w...           1   \n",
       "2        He was so amazed that he forgot to thank me .           0   \n",
       "3    Marie is so amazed that he came up with that s...           2   \n",
       "4    Her uncle was so angry that he sent her to liv...           0   \n",
       "..                                                 ...         ...   \n",
       "318  I was so skeptical that I used frozen blueberr...           0   \n",
       "319  I was so startled that I began to laugh and al...           0   \n",
       "320  I am so startled that no one seemed to have a ...           2   \n",
       "321  The character Kate was so uncertain that her b...           0   \n",
       "322  I am so uncertain that there were really enoug...           1   \n",
       "\n",
       "                                             embedding  \n",
       "0    [-0.02166154235601425, -0.006525898817926645, ...  \n",
       "1    [-0.00969354435801506, -0.0024283453822135925,...  \n",
       "2    [-0.015139495022594929, -0.0003121292102150619...  \n",
       "3    [-0.01651465706527233, 0.002538929460570216, 0...  \n",
       "4    [0.003403774229809642, -0.01876366324722767, -...  \n",
       "..                                                 ...  \n",
       "318  [0.0014925083378329873, -0.03001386672258377, ...  \n",
       "319  [-0.03688151389360428, -0.010876656509935856, ...  \n",
       "320  [-0.015010016970336437, -0.031310923397541046,...  \n",
       "321  [-0.013836754485964775, 0.004289925564080477, ...  \n",
       "322  [-0.010967683047056198, -0.0024471436627209187...  \n",
       "\n",
       "[323 rows x 5 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"embedding\"] = [get_embeddings_for_sentence(s, cache_path) for s in df[\"sentence\"]]\n",
    "print(len(df[\"embedding\"]))\n",
    "# print(list(df[\"embedding\"]))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tiktoken\n",
    "\n",
    "def bow_baseline(train_sentences, test_sentences, train_labels, test_labels):\n",
    "    # Tokenize sentences\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "    \n",
    "    # Build bag-of-words representation\n",
    "    train_bows = [Counter(encoding.encode(sentence)) for sentence in train_sentences]\n",
    "    test_bows = [Counter(encoding.encode(sentence)) for sentence in test_sentences]\n",
    "    \n",
    "    # Define a global vocabulary - a set of all unique tokens in the training set\n",
    "    vocabulary = set()\n",
    "    for bow in train_bows:\n",
    "        vocabulary.update(bow.keys())\n",
    "    vocab_list = sorted(list(vocabulary))\n",
    "    vocab_index = {token: i for i, token in enumerate(vocab_list)}\n",
    "    \n",
    "    # Convert Counters to arrays\n",
    "    bow_matrix = lambda bow : np.array([bow.get(token, 0) for token in vocab_list])  # Use this function to convert bow into matrix form\n",
    "    \n",
    "    train_bow_matrix = np.array([bow_matrix(bow) for bow in train_bows])\n",
    "    test_bow_matrix = np.array([bow_matrix(bow) for bow in test_bows])\n",
    "    \n",
    "    # Train logistic regression classifier\n",
    "    clf = LogisticRegression().fit(train_bow_matrix, train_labels)\n",
    "    \n",
    "    # Calculate and return train and test accuracies\n",
    "    train_accuracy = accuracy_score(train_labels, clf.predict(train_bow_matrix))\n",
    "    test_accuracy = accuracy_score(test_labels, clf.predict(test_bow_matrix))\n",
    "    return train_accuracy, test_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from scipy import stats\n",
    "\n",
    "def get_lr_acc(embeddings, labels, prepositions, sentence_texts, flag=True):\n",
    "\n",
    "    accuracies = []\n",
    "    weights = []\n",
    "\n",
    "    test_accuracies = []\n",
    "    bow_baseline_test_accuracies = []\n",
    "\n",
    "    for preposition in set(prepositions):\n",
    "        # Create a boolean mask for selecting sentences with this preposition\n",
    "        mask = [preposition == p for p in prepositions]\n",
    "\n",
    "        # Split the data into a training set and a test set\n",
    "        X_train = [e for i, e in enumerate(embeddings) if not mask[i]]\n",
    "        y_train = [l for i, l in enumerate(labels) if not mask[i]]\n",
    "        sentence_texts_train = [t for i, t in enumerate(sentence_texts) if not mask[i]]\n",
    "       \n",
    "        X_test = [e for i, e in enumerate(embeddings) if mask[i]]\n",
    "        y_test = [l for i, l in enumerate(labels) if mask[i]]\n",
    "        sentence_texts_test = [t for i, t in enumerate(sentence_texts) if mask[i]]\n",
    "\n",
    "        if flag:\n",
    "            sentence_texts_test_o = df_3[\"sentence\"]\n",
    "            X_test_o = [get_embeddings_for_sentence(s, cache_path) for s in sentence_texts_test_o]\n",
    "            y_test_o = np.array([0]*len(X_test_o))\n",
    "\n",
    "\n",
    "        clf = LogisticRegression(solver='liblinear')\n",
    "\n",
    "        if len(set(y_train)) > 1:\n",
    "\n",
    "            # Balance the dataset\n",
    "            resampler = RandomUnderSampler(sampling_strategy='majority')\n",
    "            X_train_resampled, y_train_resampled = resampler.fit_resample(np.array(X_train), np.array(y_train))\n",
    "\n",
    "            # Train the model on the resampled data\n",
    "            clf.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "            # Compute the training accuracy\n",
    "            train_predictions = clf.predict(X_train_resampled)\n",
    "            train_accuracy = accuracy_score(y_train_resampled, train_predictions)\n",
    "\n",
    "            # Compute the test accuracy\n",
    "            test_predictions = clf.predict(X_test)\n",
    "            test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "            \n",
    "            if flag:\n",
    "                # Compute the test accuracy of only causal excess\n",
    "                test_predictions_o = clf.predict(X_test_o)\n",
    "                test_accuracy_o = accuracy_score(y_test_o, test_predictions_o)\n",
    "\n",
    "            # Compute the majority class accuracy on the resampled data\n",
    "            majority_class = Counter(y_train_resampled).most_common(1)[0][0]\n",
    "            majority_class_predictions = [majority_class] * len(y_test)\n",
    "            majority_class_accuracy = accuracy_score(y_test, majority_class_predictions)\n",
    "\n",
    "            # Use resampled data in bow_baseline\n",
    "            sentence_texts_train_resampled = [t for i, t in enumerate(sentence_texts_train) if i in resampler.sample_indices_]\n",
    "            bow_baseline_train_acc, bow_baseline_test_acc = bow_baseline(sentence_texts_train_resampled, sentence_texts_test, y_train_resampled, y_test)\n",
    "\n",
    "        if flag:\n",
    "            accuracies.append((train_accuracy, test_accuracy, majority_class_accuracy, bow_baseline_train_acc, bow_baseline_test_acc, test_accuracy_o))\n",
    "        else:\n",
    "            accuracies.append((train_accuracy, test_accuracy, majority_class_accuracy, bow_baseline_train_acc, bow_baseline_test_acc))\n",
    "\n",
    "        weights.append(sum(mask))\n",
    "\n",
    "        test_accuracies.append(test_accuracy)\n",
    "        bow_baseline_test_accuracies.append(bow_baseline_test_acc)\n",
    "\n",
    "    accuracies = np.array(accuracies)\n",
    "    weights = np.array(weights)\n",
    "    mean_accuracies = np.average(accuracies, axis=0, weights=weights)\n",
    "    std_accuracies = np.sqrt(np.average((accuracies - mean_accuracies)**2, axis=0, weights=weights))\n",
    "\n",
    "    t_stat, p_value = stats.ttest_rel(test_accuracies, bow_baseline_test_accuracies)\n",
    "\n",
    "    result = {\n",
    "        'mean_train_accuracy': mean_accuracies[0],\n",
    "        'mean_test_accuracy': mean_accuracies[1],\n",
    "        'mean_majority_class_accuracy': mean_accuracies[2],\n",
    "        'mean_bow_baseline_train_accuracy': mean_accuracies[3],\n",
    "        'mean_bow_baseline_test_accuracy': mean_accuracies[4],\n",
    "        'std_train_accuracy': std_accuracies[0],\n",
    "        'std_test_accuracy': std_accuracies[1],\n",
    "        'std_majority_class_accuracy': std_accuracies[2],\n",
    "        'std_bow_baseline_train_accuracy': std_accuracies[3],\n",
    "        'std_bow_baseline_test_accuracy': std_accuracies[4]\n",
    "    }\n",
    "\n",
    "    result['t_stat'] = t_stat\n",
    "    result['p_value'] = p_value\n",
    "    \n",
    "    if flag:\n",
    "        result['mean_test_accuracy_only_causal_excess'] = mean_accuracies[5]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'causal excess': 0, 'non-causal': 1, 'causal': 2, 'only causal excess': 3}\n",
    "df_0 = df[df[\"gold_label\"] == 0] # 74\n",
    "df_1 = df[df[\"gold_label\"] == 1] # 53\n",
    "df_2 = df[df[\"gold_label\"] == 2] # 21\n",
    "df_3 = df[df[\"gold_label\"] == 3] # 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsample_factor = len(df_0) // len(df_2)\n",
    "\n",
    "# Duplicate the original DataFrame by the upsample factor\n",
    "df_upsampled = pd.concat([df_2] * upsample_factor, ignore_index=True)\n",
    "\n",
    "# Calculate the remaining rows needed to reach 74 and append them (you may need to customize this part)\n",
    "remaining_rows = len(df_0) - len(df_upsampled)\n",
    "if remaining_rows > 0:\n",
    "    df_upsampled_2 = pd.concat([df_upsampled, df_2.iloc[:remaining_rows]], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "causal excess vs. causal:\n",
      "mean_train_accuracy: 0.9006317201271333\n",
      "mean_test_accuracy: 0.7882882882882883\n",
      "mean_majority_class_accuracy: 0.5\n",
      "mean_bow_baseline_train_accuracy: 1.0\n",
      "mean_bow_baseline_test_accuracy: 0.7297297297297297\n",
      "std_train_accuracy: 0.0062946372195527974\n",
      "std_test_accuracy: 0.27982364847742985\n",
      "std_majority_class_accuracy: 0.20320258682234163\n",
      "std_bow_baseline_train_accuracy: 0.0\n",
      "std_bow_baseline_test_accuracy: 0.3071992812905859\n",
      "t_stat: 1.6245979503500416\n",
      "p_value: 0.10713652613671516\n",
      "mean_test_accuracy_only_causal_excess: 0.8189724377843188\n"
     ]
    }
   ],
   "source": [
    "result_excess_causal = get_lr_acc(list(df_0[\"embedding\"]) + list(df_upsampled_2[\"embedding\"]), \n",
    "                                  np.concatenate((df_0[\"gold_label\"],df_upsampled_2[\"gold_label\"])), \n",
    "                                  list(df_0[\"adjective\"]) + list(df_upsampled_2[\"adjective\"]), \n",
    "                                  list(df_0[\"sentence\"]) + list(df_upsampled_2[\"sentence\"]))\n",
    "print(\"causal excess vs. causal:\")\n",
    "for key, value in result_excess_causal.items():\n",
    "    print(f'{key}: {value}')\n",
    "result_excess_causal[\"perceptron\"] = \"causal excess vs. causal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsample_factor = len(df_0) // len(df_1)\n",
    "\n",
    "# Duplicate the original DataFrame by the upsample factor\n",
    "df_upsampled = pd.concat([df_1] * upsample_factor, ignore_index=True)\n",
    "\n",
    "# Calculate the remaining rows needed to reach 74 and append them (you may need to customize this part)\n",
    "remaining_rows = len(df_0) - len(df_upsampled)\n",
    "if remaining_rows > 0:\n",
    "    df_upsampled_1 = pd.concat([df_upsampled, df_1.iloc[:remaining_rows]], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "causal excess vs. non-causal:\n",
      "mean_train_accuracy: 0.9017651721780163\n",
      "mean_test_accuracy: 0.7927927927927928\n",
      "mean_majority_class_accuracy: 0.5\n",
      "mean_bow_baseline_train_accuracy: 1.0\n",
      "mean_bow_baseline_test_accuracy: 0.7297297297297297\n",
      "std_train_accuracy: 0.006445417821724725\n",
      "std_test_accuracy: 0.2805116662511761\n",
      "std_majority_class_accuracy: 0.20320258682234163\n",
      "std_bow_baseline_train_accuracy: 0.0\n",
      "std_bow_baseline_test_accuracy: 0.3071992812905859\n",
      "t_stat: 1.7070599675815914\n",
      "p_value: 0.09065807149069363\n",
      "mean_test_accuracy_only_causal_excess: 0.8193292302203189\n"
     ]
    }
   ],
   "source": [
    "result_excess_noncausal = get_lr_acc(list(df_0[\"embedding\"]) + list(df_upsampled_2[\"embedding\"]), \n",
    "                                  np.concatenate((df_0[\"gold_label\"],df_upsampled_2[\"gold_label\"])), \n",
    "                                  list(df_0[\"adjective\"]) + list(df_upsampled_2[\"adjective\"]), \n",
    "                                  list(df_0[\"sentence\"]) + list(df_upsampled_2[\"sentence\"]))\n",
    "print(\"causal excess vs. non-causal:\")\n",
    "for key, value in result_excess_noncausal.items():\n",
    "    print(f'{key}: {value}')\n",
    "result_excess_noncausal[\"perceptron\"] = \"causal excess vs. non-causal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsample_factor = len(df_2) // len(df_1)\n",
    "\n",
    "# Duplicate the original DataFrame by the upsample factor\n",
    "df_upsampled = pd.concat([df_1] * upsample_factor, ignore_index=True)\n",
    "\n",
    "# Calculate the remaining rows needed to reach 74 and append them (you may need to customize this part)\n",
    "remaining_rows = len(df_2) - len(df_upsampled)\n",
    "if remaining_rows > 0:\n",
    "    df_upsampled_1 = pd.concat([df_upsampled, df_1.iloc[:remaining_rows]], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "causal vs. non-causal:\n",
      "mean_train_accuracy: 0.9604954143584283\n",
      "mean_test_accuracy: 0.7710843373493976\n",
      "mean_majority_class_accuracy: 0.5\n",
      "mean_bow_baseline_train_accuracy: 1.0\n",
      "mean_bow_baseline_test_accuracy: 0.4578313253012048\n",
      "std_train_accuracy: 0.004863053449313866\n",
      "std_test_accuracy: 0.41472305187279085\n",
      "std_majority_class_accuracy: 0.4954613281688525\n",
      "std_bow_baseline_train_accuracy: 0.0\n",
      "std_bow_baseline_test_accuracy: 0.4936635803712797\n",
      "t_stat: 7.62192979055534\n",
      "p_value: 9.839896997726459e-12\n"
     ]
    }
   ],
   "source": [
    "result_causal_noncausal = get_lr_acc(list(df_2[\"embedding\"]) + list(df_upsampled_1[\"embedding\"]), \n",
    "                                  np.concatenate((df_2[\"gold_label\"],df_upsampled_1[\"gold_label\"])), \n",
    "                                  list(df_2[\"adjective\"]) + list(df_upsampled_1[\"adjective\"]), \n",
    "                                  list(df_2[\"sentence\"]) + list(df_upsampled_1[\"sentence\"]),\n",
    "                                    False)\n",
    "print(\"causal vs. non-causal:\")\n",
    "for key, value in result_causal_noncausal.items():\n",
    "    print(f'{key}: {value}')\n",
    "result_causal_noncausal[\"perceptron\"] = \"causal vs. non-causal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsample_factor = len(df_0) // len(df_3)\n",
    "\n",
    "# Duplicate the original DataFrame by the upsample factor\n",
    "df_upsampled = pd.concat([df_3] * upsample_factor, ignore_index=True)\n",
    "\n",
    "# Calculate the remaining rows needed to reach 74 and append them (you may need to customize this part)\n",
    "remaining_rows = len(df_0) - len(df_upsampled)\n",
    "if remaining_rows > 0:\n",
    "    df_upsampled_3 = pd.concat([df_upsampled, df_3.iloc[:remaining_rows]], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_excess_onlyexcess = get_lr_acc(list(df_0[\"embedding\"]) + list(df_upsampled_3[\"embedding\"]), \n",
    "#                                   np.concatenate((df_0[\"gold_label\"],df_upsampled_3[\"gold_label\"])), \n",
    "#                                   list(df_0[\"adjective\"]) + list(df_upsampled_3[\"adjective\"]), \n",
    "#                                   list(df_0[\"sentence\"]) + list(df_upsampled_3[\"sentence\"]),\n",
    "#                                  False)\n",
    "# print(\"causal excess vs. only causal excess:\")\n",
    "# for key, value in result_excess_onlyexcess.items():\n",
    "#     print(f'{key}: {value}')\n",
    "# result_excess_onlyexcess[\"perceptron\"] = \"causal excess vs. only causal excess\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "causal excess vs. licensed causal/noncausal:\n",
      "mean_train_accuracy: 0.8722580378543686\n",
      "mean_test_accuracy: 0.7567567567567568\n",
      "mean_majority_class_accuracy: 0.5\n",
      "mean_bow_baseline_train_accuracy: 1.0\n",
      "mean_bow_baseline_test_accuracy: 0.7297297297297297\n",
      "std_train_accuracy: 0.005933923974563531\n",
      "std_test_accuracy: 0.2673263085577981\n",
      "std_majority_class_accuracy: 0.0\n",
      "std_bow_baseline_train_accuracy: 0.0\n",
      "std_bow_baseline_test_accuracy: 0.31324934505790486\n",
      "t_stat: 0.7370138185751772\n",
      "p_value: 0.462697405378074\n",
      "mean_test_accuracy_only_causal_excess: 0.6035144054946032\n"
     ]
    }
   ],
   "source": [
    "result_excess_others = get_lr_acc(list(df_0[\"embedding\"]) + list(df_1[\"embedding\"]) + list(df_2[\"embedding\"]), \n",
    "                                  np.concatenate((df_0[\"gold_label\"],np.array([1]*len(df_0)))), \n",
    "                                  list(df_0[\"adjective\"]) + list(df_1[\"adjective\"]) + list(df_2[\"adjective\"]), \n",
    "                                  list(df_0[\"sentence\"]) + list(df_1[\"sentence\"]) + list(df_2[\"sentence\"]))\n",
    "print(\"causal excess vs. licensed causal/noncausal:\")\n",
    "for key, value in result_excess_others.items():\n",
    "    print(f'{key}: {value}')\n",
    "result_excess_others[\"perceptron\"] = \"causal excess vs. licensed causal/noncausal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [result_excess_causal, result_excess_noncausal, result_causal_noncausal, result_excess_others]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame(dfs)\n",
    "for column in new_df.columns:\n",
    "    # Check if the column name contains \"accuracy\"\n",
    "    if \"accuracy\" in column:\n",
    "        # Apply the desired formatting to the column\n",
    "        new_df[column] = new_df[column].apply(lambda x: f\"{x * 100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to an Excel file\n",
    "new_df.to_excel(\"gpt_perceptron_accuracy.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "b26bbf8db02da89a16f0bf6b34287d6bafac7092a1e865ceadcdcc00f0f331ad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
